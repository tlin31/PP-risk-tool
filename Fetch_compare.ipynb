{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-20T21:33:52.782137Z",
     "iopub.status.busy": "2020-08-20T21:33:52.780838Z",
     "iopub.status.idle": "2020-08-20T21:33:52.878801Z",
     "shell.execute_reply": "2020-08-20T21:33:52.878038Z",
     "shell.execute_reply.started": "2020-08-20T21:33:52.782023Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from itertools import repeat\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "%config PPMagics.autolimit=0\n",
    "%config SqlMagic.autolimit = 0\n",
    "from IPython.display import display, HTML\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from time import sleep\n",
    "Multiple_Regex=True\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from itertools import repeat\n",
    "from multiprocessing.pool import ThreadPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_time_in_secs = 5.0\n",
    "bundle_size = 3\n",
    "\n",
    "token = 'eyJhbGciOiJSUzI1NiIsImtpZCI6IkN1c3RvbUpXVHNpZ25pbmciLCJwaS5hdG0iOiJzdTk0In0.eyJzY29wZSI6InBheXBhbDJmYSBwcm9maWxlIiwiY2xpZW50X2lkIjoiQ0FMX0F1dGhlbnRpY2F0b3IiLCJHVUlEIjoiTkF3eUNYcmpvM2Q2aFA0Q3FDQm5aek42VW5CUGVTS1oiLCJpc3MiOiJodHRwczovL3Nzby5wYXlwYWxjb3JwLmNvbSIsIlVzZXJuYW1lIjoidGlhbGluIiwiZXhwIjoxNTk3ODc1ODM0fQ.Eq9qPxzmk8AxLuy93RAEwGA4TnoSLb8V5ZHe5p0MlbjU1TYu36pxlmC87jyyDJT62g4MNWnT6Hap4qjx-pqXOVcXXSnm5CTIxnbeanTtR0zUcrD5LHFD0ItCyj7naPXN2CrACss8B-xnLZSry_7qhlonjnOGhZ0pYY9sGzTusAToEuO7WEmHQZI5bUoveJEq88pSaasSrNkmI4LCESSaHZLaur5RTMxC9bYIWi4yHxYh_idJmc5uMmTHB1zoyZbQyg_2KdaICi48FG-ktR6e3I-Y-dVrZ11nMw089rtu18wzdSYnd0oePZ-l-uNx9Pb-r5DzIjSCzWsarsgAso7wkg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_and_compare (typeA_machine_name,typeB_machine_name,start_date,end_date,typeA_pool_name,typeB_pool_name,typeA_data_center,typeB_data_center):\n",
    "    \n",
    "    primary_df,all_corr_ids = get_Primary_Serv_CAL_records(typeA_data_center,start_date,end_date,typeA_pool_name,typeA_machine_name, delay_time_in_secs)\n",
    "\n",
    "    num_corr_ids = len(all_corr_ids)\n",
    "    counter = 0\n",
    "    \n",
    "    header_list_primary = [\"Primary Service\",\"Secondary Service\",\"DB-related\",\"Issue Details\",\"corr_id\"]\n",
    "    diff_tables = pd.DataFrame(columns=header_list_primary)\n",
    "    while counter + bundle_size < num_corr_ids:\n",
    "        search_corr_ids = all_corr_ids[counter:counter + bundle_size]\n",
    "        temp_df = get_Secondary_And_Compare(counter,counter+bundle_size,typeB_data_center,all_corr_ids,start_date,end_date,typeB_pool_name,typeB_machine_name,delay_time_in_secs,primary_df,typeA_pool_name)\n",
    "#         print(temp_df)\n",
    "        diff_tables.append(temp_df)\n",
    "        counter += bundle_size\n",
    "    \n",
    "    # get the rest \n",
    "    search_corr_ids = all_corr_ids[counter:num_corr_ids]\n",
    "    diff_tables.append(get_Secondary_And_Compare(counter,counter+bundle_size,typeB_data_center,all_corr_ids,start_date,end_date,typeB_pool_name,typeB_machine_name,delay_time_in_secs,primary_df,typeA_pool_name))\n",
    "\n",
    "\n",
    "    return diff_tables\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    startDate = '2020/08/18-15:00:00'\n",
    "    endDate = '2020/08/18-15:15:00'\n",
    "    corrID = 'ac65f81dd963e'\n",
    "    machine_name = 'all'\n",
    "    server1 = 'acserv_acriskserv'    \n",
    "    combine_and_compare ('','',startDate,endDate,server1,server1,'','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-20T23:49:29.470637Z",
     "iopub.status.busy": "2020-08-20T23:49:29.470245Z",
     "iopub.status.idle": "2020-08-20T23:57:04.593120Z",
     "shell.execute_reply": "2020-08-20T23:57:04.592325Z",
     "shell.execute_reply.started": "2020-08-20T23:49:29.470583Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_Primary_Serv_CAL_records(data_center,start_date,end_date,pool_name,machine_name_known,delay_time_in_secs): \n",
    " \n",
    "    %config PPMagics.autolimit=0;\n",
    "    \n",
    "    time.sleep(delay_time_in_secs) \n",
    "    \n",
    "    query = \"\"\"\n",
    "paps = LOAD 'tmp' using RootTransactionLoader('{data_center_Name}', 'paypal','{pool}', '{machine_name}', '{startDate}', '{endDate}', '100')\n",
    "    as (env:bytearray, pool:bytearray, machine:bytearray, threadId:int, calClass:bytearray, time:long, type:bytearray,  name:bytearray, status:bytearray, duration:float, dataMap:map[], records:bag{{(record:bytearray)}}, key:bytearray, colo:bytearray);\n",
    "paps =FOREACH paps {{\n",
    "           PXP  = FILTER records BY (record MATCHES '.*\\tEXEC\\t.*'OR record MATCHES '.*\\tBIZ\\t.*'OR record MATCHES '.*\\tSTART_DML\\t.*');\n",
    "       GENERATE dataMap#'corr_id_' as (corrID:bytearray),PXP;\n",
    "         }}\n",
    "group_data = GROUP paps by corrID;\n",
    "T = LIMIT group_data 100;\n",
    "\n",
    "store T into '$output' using PigStorage(',');\n",
    "\"\"\".format(data_center_Name = data_center,pool=pool_name, machine_name = machine_name_known,startDate= start_date, endDate =end_date);\n",
    "\n",
    "    \n",
    "    CAL_rows =%calpig $query;\n",
    "    CAL_df=pd.DataFrame(CAL_rows);\n",
    "\n",
    "    validRows = pd.DataFrame(columns=['corr_id','data'])\n",
    "    all_corr_ids=[]\n",
    "    \n",
    "    counter = 0\n",
    "    for index, row in CAL_df.iterrows():\n",
    "        corr_id = str(row).split()[1].split(',')[0]\n",
    "        data = str(row).split()[1]\n",
    "\n",
    "        if \"no\" not in corr_id: \n",
    "            all_corr_ids.append(corr_id)\n",
    "            new_row = {'corr_id':corr_id,'data':data}\n",
    "            validRows = validRows.append(new_row, ignore_index=True)\n",
    "\n",
    "    \n",
    "    validRows.to_csv('primary_rows_final.csv',index=False)\n",
    "\n",
    "    return validRows,all_corr_ids;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-21T00:20:08.495467Z",
     "iopub.status.busy": "2020-08-21T00:20:08.494695Z",
     "iopub.status.idle": "2020-08-21T00:20:19.311218Z",
     "shell.execute_reply": "2020-08-21T00:20:19.310127Z",
     "shell.execute_reply.started": "2020-08-21T00:20:08.495390Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_Secondary_And_Compare(row_start,row_end,data_center,all_corr_ids,start_date,end_date,pool_name,machine_name_known,delay_time_in_secs,primary_df,typeA_pool_name):\n",
    "\n",
    "        \n",
    "    header_list = [\"corr_id\", \"data\"]\n",
    "    \n",
    "    # skip the column name\n",
    "    row_start +=1\n",
    "    primary_df = pd.read_csv(\"primary_rows_f.csv\",skiprows=row_start,nrows=row_end+2,names=header_list) \n",
    "\n",
    "    \n",
    "    pig_Scrip = []\n",
    "    pig_Union_stmt = []\n",
    "    pig_Union_stmt.append('T = UNION')\n",
    "    index_str = ''\n",
    "    group_name  = ''\n",
    "    for index, corr_id in enumerate(all_corr_ids):\n",
    "        index_str = str(index)\n",
    "        group_name = 'group' + index_str\n",
    "        table_Name = 'table' + index_str\n",
    "    \n",
    "        query = \"\"\"\n",
    "{tableName} = LOAD 'tmp' using RootTransactionLoader('{data_center_Name}', 'paypal','{pool}', '{machine_name}', '{startDate}', '{endDate}', '100','null','null','corr_id_','{corrID}') as (env:bytearray, pool:bytearray, machine:bytearray, threadId:int, calClass:bytearray, time:long, type:bytearray,  name:bytearray, status:bytearray, duration:float, dataMap:map[], records:bag{{(record:bytearray)}}, key:bytearray, colo:bytearray);\n",
    "        \n",
    "paps =FOREACH {tableName} {{\n",
    "           PXP  = FILTER records BY (record MATCHES '.*\\tEXEC\\t.*'OR record MATCHES '.*\\tBIZ\\t.*'OR record MATCHES '.*\\tSTART_DML\\t.*');\n",
    "       GENERATE dataMap#'corr_id_' as (corrID:bytearray),PXP;\n",
    "         }}\n",
    "         \n",
    "{groupData} = GROUP paps by corrID;\n",
    "\n",
    "\"\"\".format(tableName = table_Name, groupData = group_name, data_center_Name = data_center,pool=pool_name, machine_name = machine_name_known,startDate= start_date, endDate =end_date,corrID = corr_id);\n",
    "\n",
    "        pig_Scrip.append(query)\n",
    "        pig_Union_stmt.append(group_name + ',')\n",
    "   \n",
    "\n",
    "    pig_Union_stmt = ' '.join([str(elem) for elem in pig_Union_stmt]) \n",
    "    pig_Union_stmt = pig_Union_stmt[:-1] + \";\"\n",
    "    pig_Scrip.append(pig_Union_stmt)\n",
    "\n",
    "    pig_Scrip.append(\"store T into '$output' using PigStorage(',');\")\n",
    "    pig_Scrip = ' '.join([str(elem) for elem in pig_Scrip]) \n",
    "\n",
    "#     print(pig_Scrip)\n",
    "    \n",
    "\n",
    "#     CAL_rows_secondary = %calpig $query;\n",
    "#     secondary_data = pd.DataFrame(CAL_rows_secondary);\n",
    "\n",
    "#     valid_second_rows = pd.DataFrame(columns=['corr_id','data'])\n",
    "    \n",
    "#     for index, row in secondary_data.iterrows():\n",
    "#         corr_id = str(row).split()[1].split(',')[0]\n",
    "#         data = str(row).split()[1]\n",
    "\n",
    "#         if \"no\" not in corr_id: \n",
    "#             new_row = {'corr_id':corr_id,'data':data}\n",
    "#             valid_second_rows = valid_second_rows.append(new_row, ignore_index=True)\n",
    "\n",
    "    # For testing purpose, directly read from csv file    \n",
    "    valid_second_rows = pd.read_csv(\"primary_rows_f.csv\",skiprows=row_start+2,nrows=row_end+2,names=header_list) \n",
    "    \n",
    "    # merge primary and secondary data, check for whether the data are same, leave the one that don't match\n",
    "    diff_table = pd.merge(primary_df, valid_second_rows, on=['corr_id'], how='outer')\n",
    "    diff_table['records_same_flag'] = np.where((diff_table['data_x'] == diff_table['data_y']),  True, False)\n",
    "    diff_table.insert (0, \"Primary Service\", typeA_pool_name)\n",
    "    diff_table.insert (1, \"Secondary Service\", pool_name)\n",
    "    \n",
    "    # Note: for the acrisk serv, all the differences are database-related\n",
    "    diff_table.insert (2, \"DB-related\", 'Yes')\n",
    "    diff_table.insert (3, \"Issue Details\", diff_table.loc[:,'data_x':'data_y'].fillna(method='ffill',axis=1)['data_y'])\n",
    "#     diff_table.insert (3, \"TYPE\", np.where(\"EXEC\"in str(diff_table['Issue Details']),'EXEC',np.where('BIZ' in diff_table['Issue Details'], 'BIZ', np.where('START_DML' in diff_table['Issue Details'], 'START_DML', 'New record not found in another service'))))\n",
    "\n",
    "                                     \n",
    "    diff_table = diff_table[diff_table.records_same_flag != True]\n",
    "    diff_table = diff_table.drop('data_x', 1)\n",
    "    diff_table = diff_table.drop('data_y', 1)\n",
    "    diff_table = diff_table.drop('records_same_flag', 1)\n",
    "\n",
    "    diff_table.to_csv('diff_tables.csv',index=False)\n",
    "#     print(diff_table)\n",
    "    return diff_table\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     start_date = '2020/08/18-15:00:00'\n",
    "#     end_date = '2020/08/18-15:15:00'\n",
    "#     machine_name_known = ''\n",
    "#     data_center=''\n",
    "#     server1 = 'type b'  \n",
    "#     typeA_pool_name = 'acserv_acriskserv'  \n",
    "#     row_start = 0\n",
    "#     row_end = 3\n",
    "\n",
    "#     all_corr_ids = ['10023d1051f9', '1003cffc0d62', '1004a9750f17', '100d84c7397e', '10102689e97b', '101356f2d4d6', '10135a988121', '10135b786960', '101694fc778c', '10191839cf97', '1019198bcef4', '10191fbf34c9', '1020920f9f06', '1021bef719a3']\n",
    "#     header_list = [\"corr_id\", \"data\"]\n",
    "\n",
    "#     primary_df = pd.DataFrame(columns=header_list)\n",
    "\n",
    "#     diff = get_Secondary_And_Compare(row_start,row_end,data_center,all_corr_ids,start_date,end_date,server1,machine_name_known,delay_time_in_secs,primary_df,typeA_pool_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqlStmt(sqlCode):\n",
    "\n",
    "    max_row_values = 500\n",
    "    headers = dict()\n",
    "    headers['Accept-Encoding'] = 'gzip'\n",
    "    headers['X-CAL-AUTH-TOKEN'] = token\n",
    "    \n",
    "    # https://engineering.paypalcorp.com/confluence/display/CAL/Search+REST+Interface\n",
    "#     $ curl \"http://calhadoop-vip-a.slc.paypal.com/logview/pool/paymentserv/sql?codes=392053925,2637078142,3998637305\"\n",
    "\n",
    "    while ctr<max_row_values:\n",
    "        request_Endpoint='http://calhadoop-vip-a.slc.paypal.com/logview/pool/paymentserv/sql?codes={sqlCode1}'.format(sqlCode1=sqlCode1);\n",
    "        headers = {'Accept-Encoding': 'gzip','X-CAL-AUTH-TOKEN':token} \n",
    "        request=requests_retry_session();\n",
    "        response = request.get(url = request_Endpoint, headers = headers) \n",
    "        response_as_json = response.json();\n",
    "#         print(response_as_json)\n",
    "        \n",
    "\n",
    "    return response_as_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "Python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}